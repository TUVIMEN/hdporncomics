#!/usr/bin/env python
# by Dominik Stanis≈Çaw Suchora <suchora.dominik7@gmail.com>
# License: GNU GPLv3

import sys
import os
import json
import re
import hashlib
from datetime import datetime
from queue import Queue
from pathlib import Path
from itertools import chain

from hdporncomics import hdporncomics, RequestError, AuthorizationError


def strtosha256(string: str | bytes) -> str:
    if isinstance(string, str):
        string = string.encode()

    return hashlib.sha256(string).hexdigest()


def urlvalid(
    url,
    methods=lambda x: x.lower() in ["http", "https"],
    domain=lambda x: len(x) > 0,
    rest=lambda x: True,
):
    if not isinstance(url, str):
        return False
    r = re.match(r"^([a-zA-Z]+)://(([a-z0-9A-Z_-]+\.)+[a-zA-Z]+)((/|\?|$).*)", url)
    if r is None:
        return False

    if not methods(r[1]):
        return False
    if not domain(r[2]):
        return False
    if not rest(r[4]):
        return False
    return True


class hdporncomicsItemExtractor:
    def __init__(self, hdpo, path, domain):
        self.hdpo = hdpo
        self.domain = domain

        if not os.path.exists(path):
            os.mkdir(path)
        elif not os.path.isdir(path):
            raise Exception('given path "{}" is not a directory'.format(path))

        self.path = Path(path)

    def def_matcher(self, restregex, domain=None):
        return lambda x: urlvalid(
            x,
            rest=lambda y: re.fullmatch(restregex, y) is not None,
            domain=lambda y: len(y) > 0 if domain is None else (y == domain),
        )

    def match(self, url):
        return False

    def get_item_from_file(self, fname):
        with open(fname, "r") as f:
            return json.load(f)

    def get_item_from_url(self, url):
        return {}

    def get_item(self, url, fname="", force=False):
        if len(fname) > 0:
            return self.get_item_from_file(fname)
        else:
            if not force:
                fname = self.exists(url)
                if fname is not None:
                    return self.get_item_from_file(fname)

            return self.get_item_from_url(url)

    def discover(self, item, links):
        pass

    def get_update_factor_field(self, item):
        return 0

    def get_update_factor(self, fname):
        item = self.get_item("", fname=fname)
        if item is None:
            return
        return self.get_update_factor_field(item)

    def mkpath(self, path):
        return str(self.path / path)

    def item_path(self, url):
        return self.mkpath(strtosha256(url))

    def exists(self, url):
        fname = self.item_path(url)

        if not os.path.isfile(fname):
            return None

        if os.path.getsize(fname) == 0:
            return None

        return fname

    def check_update_factor(self, fname, update_factor, update_value):
        if update_factor == 0 or update_value == 0:
            return False

        if update_factor == -1:
            return True

        r = self.get_update_factor(fname)
        if r is not None and r * update_factor > update_value:
            return False

        return True

    def check(self, url, update_factor, update_value):
        """
        check if item should be updated
        """

        if not self.match(url):
            return None
        if (fname := self.exists(url)) is None:
            return True
        else:
            return self.check_update_factor(fname, update_factor, update_value)

    def add(self, url, links):
        item = self.get(url, force=True)
        if item is None:
            return None

        with open(self.item_path(url), "w") as f:
            json.dump(item, f)

        self.discover(item, links)
        return True

    def get(self, url, fname="", force=False):
        if not self.match(url):
            return
        return self.get_item(url, fname=fname, force=force)

    def update_links_page(self, unscraped, page):
        return []

    def update_links(
        self,
        unscraped,
        itern,
        checker,
        notfound=5,
        notfound_pages=3,
        update_factor=1.3,
        maxpages=-1,
    ):
        if maxpages == 0:
            return

        page = 1
        notfound_all = 0
        notfound_prev = Queue()

        try:
            for i in itern:
                notfound_onpage = 0

                for j, c in self.update_links_page(unscraped, i):
                    if checker(j, update_factor, c):
                        notfound_onpage += 1
                        unscraped.add(j)

                notfound_all += notfound_onpage
                if notfound_pages > 0:
                    notfound_prev.put(notfound_onpage)

                    if page >= notfound_pages:
                        if notfound_all < notfound:
                            break
                        notfound_all -= notfound_prev.get()

                if maxpages != -1 and page >= maxpages:
                    break
                page += 1
        except RequestError:
            pass

    def get_new(self, unscraped, checker):
        pass


class comicItemExtractor(hdporncomicsItemExtractor):
    def __init__(self, *args):
        super().__init__(*args)

        self.match = self.def_matcher(r"/[^/]+(/(#.*)?)?", domain=self.domain)

    def update_links_page(self, unscraped, page):
        for i in page["posts"]:
            url = i["link"]
            if url in unscraped:
                continue

            yield url, i["views"]

    def get_item_from_url(self, url):
        return self.hdpo.get_comic(url, comments=-1)

    def discover(self, item, links):
        for i in item["related"]:
            for j in i["items"]:
                links.add(j["link"])

    def get_update_factor_field(self, item):
        return item["views"]

    def get_new(self, unscraped, checker):
        self.update_links(
            unscraped,
            self.hdpo.get_new(),
            checker,
        )


class gaycomicItemExtractor(comicItemExtractor):
    def __init__(self, *args):
        super().__init__(*args)

        self.match = self.def_matcher(r"/gay-manga/[^/]+(/(#.*)?)?", domain=self.domain)

    def get_new(self, unscraped, checker):
        self.update_links(
            unscraped,
            self.hdpo.get_gay(),
            checker,
        )


class manhwaItemExtractor(hdporncomicsItemExtractor):
    def __init__(self, *args):
        super().__init__(*args)

        self.match = self.def_matcher(r"/manhwa/[^/]+(/(#.*)?)?", domain=self.domain)

    def get_update_factor_field(self, item):
        return item["views"]

    def get_item_from_url(self, url):
        return self.hdpo.get_manhwa(url, comments=-1)

    def update_links_page(self, unscraped, page):
        for i in page["posts"]:
            url = i["link"]
            if url in unscraped:
                continue

            yield url, i["views"]

    def discover(self, item, links):
        for i in item["chapters"]:
            links.add(i["link"])

    def get_new(self, unscraped, checker):
        self.update_links(
            unscraped,
            self.hdpo.get_manhwas(),
            checker,
        )


class manhwachapterItemExtractor(hdporncomicsItemExtractor):
    def __init__(self, *args):
        super().__init__(*args)

        self.match = self.def_matcher(
            r"/manhwa/[^/]+/[^/]+(/(#.*)?)?", domain=self.domain
        )

    def get_update_factor_field(self, item):
        return 0

    def get_item_from_url(self, url):
        item = self.hdpo.get_manhwa_chapter(url, comments=0)
        item["comments"] = []
        return item

    def get_new(self, unscraped, checker):
        pass


class hdporncomicsExtractor:
    def __init__(self, path, **kwargs):
        if not os.path.isdir(path):
            raise Exception("expected directory")
        self.path = Path(path)

        self.links_file = self.mkpath("links")

        self.domain = "hdporncomics.com"

        self.hdpo = hdporncomics(**kwargs)

        self.items = [
            manhwachapterItemExtractor(
                self.hdpo, self.mkpath("manhwachapter"), self.domain
            ),
            manhwaItemExtractor(self.hdpo, self.mkpath("manhwa"), self.domain),
            gaycomicItemExtractor(self.hdpo, self.mkpath("gay"), self.domain),
            comicItemExtractor(self.hdpo, self.mkpath("comic"), self.domain),
        ]

    def mkpath(self, path):
        return str(self.path / path)

    def load_links(self, fname):
        links = set()

        with open(fname, "r") as f:
            for i in f:
                url = i.strip()
                links.add(url)

        return links

    def unscraped_links(self, links):
        unscraped = set()
        for i in links:
            r = self.item_matching(i)
            if r is None:
                continue
            if not r.exists(i):
                unscraped.add(i)

        return unscraped

    def item_matching(self, url):
        for i in self.items:
            if i.match(url) is True:
                return i
        return

    def items_checker(self, url, update_factor, update_value):
        for i in self.items:
            r = i.check(url, update_factor, update_value)
            if r is not None:
                return r
        return False

    def update(self):
        try:
            links = self.load_links(self.links_file)
        except FileNotFoundError:
            pass

        unscraped = self.unscraped_links(links)

        for i in self.items:
            i.get_new(unscraped, lambda x, y, z: self.items_checker(x, y, z))

        links.update(unscraped)

        return links, unscraped

    def save_links(self, links):
        with open(self.links_file, "w") as f:
            for i in links:
                f.write(i)
                f.write("\n")

    def update_all(self):
        links, unscraped = self.update()
        foundall = 0
        self.save_links(links)

        while True:
            found = 0
            for i in unscraped:
                r = self.item_matching(i)
                if r is None:
                    continue
                r.add(i, links)
                found += 1

            if found == 0:
                break

            self.save_links(links)
            unscraped = self.unscraped_links(links)
            foundall += found

        return foundall


if len(sys.argv) == 1:
    print("{} <DIR>".format(sys.argv[0]), file=sys.stderr)
    print(
        "\nA sophisticated tool for scraping the whole hdporncomics.com site",
        file=sys.stderr,
    )
    exit()

hdpoe = hdporncomicsExtractor(
    sys.argv[1], wait=1, retry=3, retry_wait=60, logger=sys.stdout
)

hdpoe.update_all()
